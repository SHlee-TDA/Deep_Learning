{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning for Mathematicians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 퍼셉트론 (Perceptron)\n",
    "\n",
    "선형회귀를 복습해보자.\n",
    "\n",
    "선형회귀 문제는 데이터 $\\left\\{(\\mathbf{x}_i, y_i) : \\mathbf{x}_i \\in \\mathbb{R}^d, y_i \\in \\mathbb{R}\\right\\}_{i=1}^N$이 주어졌을 때, 모든 $i$에 대하여 $\\hat{y}(x_i)$ 과 $y_i$의 오차가 최소가 되게 하는 모델\n",
    "$$ \\hat{y}(\\mathbf{x}) = w_0 + \\mathbf{w}^T \\mathbf{x} = w_0 + \\sum_{i=1}^d w_ix_i$$\n",
    "를 찾는 문제다. 여기서 $\\mathbf{x}$는 입력 데이터, $w_0 \\in \\mathbb{R}, \\mathbf{w} = (w_1,\\ldots, w_d)^T \\in \\mathbb{R}^d$는 찾아야 할 가중치(weight)다.\n",
    "\n",
    "인공신경망의 기본 단위가 되는 **퍼셉트론**은 이 선형회귀 모델의 변형이라고 할 수 있다.\n",
    "고전적인 퍼셉트론 모델은 다음과 같이 정의가 된다.\n",
    "$$ \\hat{y}(\\mathbf{x}) = H(w_0 + \\mathbf{w}^T\\mathbf{x}) = H\\left(w_0 + \\sum_{i=1}^d w_ix_i\\right)$$\n",
    "여기서 $H(t) = \\begin{cases} 1 & \\text{ if } t>0 \\\\ 0 & \\text{ otherwise }\\end{cases}$ 로 정의되는 *(Heaviside) step function*이다.\n",
    "\n",
    "퍼셉트론 모델의 출력은 0 또는 1의 값으로, 선형회귀와는 달리 이진분류 문제(즉, $y_i$의 값이 $0$ 또는 $1$)에 사용된다.\n",
    "선형회귀이든 선형회귀든 그 목적은 결국 원래 데이터의 정답 $y_i$와 예측 $\\hat{y}_i = \\hat{y}(\\mathbf{x}_i)$ 사이의 오차를 최소화하는 가중치 $w_0, w_1, \\ldots, w_d$를 찾는데 있다.\n",
    "\n",
    "선형회귀 모델은 데이터가 선형적인 경향성을 띌 수록, 퍼셉트론 모델은 데이터가 선형적인 결정 경계에 의해 분리가 잘 되어있을 수록 오차가 적다.\n",
    "거꾸로 말하면, 데이터가 비선형적인 경향성을 띄거나 데이터가 선형적으로 분류가 될 수 없을 경우 두 모델은 오차가 매우 커진다. \n",
    "실제 데이터는 비선형적인 경향성을 띄는 경우가 많다.\n",
    "따라서 복잡한 데이터셋에서 정답과 예측 사이의 오차를 줄이기 위해서는 비선형성이 모델에 부여될 수 있도록 해야한다.\n",
    "\n",
    "> ***Remark)*** 퍼셉트론 모델이 만드는 결정경계는 affine plane $\\left\\{ \\mathbf{x} \\in \\mathbb{R}^d : w_0 + \\mathbf{w}^T\\mathbf{x} = 0\\right\\}$ 이다. 따라서, 퍼셉트론 모델 자체는 입력 $\\mathbf{x}$에 대하여 비선형함수이지만, 그것이 생성하는 결정경계는 선형이므로 *선형분류기*라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 신경망 (Neural Network)\n",
    "\n",
    "퍼셉트론 모델에 비선형성을 부여하려면 어떻게 해야할까?\n",
    "정답은 의외로 간단한데, *여러 퍼셉트론을 층으로 쌓는 것*이다. \n",
    "\n",
    "이는 보통 아래의 그림과 같이 도식화된다.\n",
    "\n",
    "![neural_net](https://github.com/SHlee-TDA/Deep_Learning/blob/master/images/neural_net.png?raw=true)\n",
    "\n",
    "위 그림의 구조에는 총 3개의 퍼셉트론 함수가 등장한다.\n",
    "먼저 입력 $\\mathbf{x} = (x_1, x_2)^T \\in \\mathbb{R}^2$를 받는 두 개의 퍼셉트론 $z_1 = H(w_{11}x_1 + w_{12}x_2)$, $z_2 = H(w_{21}x_1 + w_{22}x_2)$가 있다. 여기서 편의상 상수항 $w_0$는 생략했다.\n",
    "그리고 최종적으로 앞선 두 퍼셉트론의 값 $\\mathbf{z} = (z_1, z_2)^T$를 입력받는 하나의 퍼셉트론 $\\hat{y} = H(s_1z_1 + s_2z_2)$가 있다.\n",
    "\n",
    "이를 행렬로 표현하면, $\\mathbf{z} = H(W\\mathbf{x})$ 이고, $\\hat{y} = H(S\\mathbf{z})$ 이므로\n",
    "$$ \\hat{y}(\\mathbf{x}) = H(S(H(W\\mathbf{x})))$$\n",
    "로 표현된다. \n",
    "여기서 $W = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22}\\end{bmatrix}$, $S = \\begin{bmatrix} s_{1} & s_{2}\\end{bmatrix}$ 이다.\n",
    "\n",
    "이처럼 퍼셉트론을 여러 층으로 쌓은 모델을 **Multi-layer Perceptron(MLP)** 이라고 부른다.\n",
    "여기서 $\\mathbf{x} = (x_1, x_2)^T$를 **입력층**, $\\mathbf{z} = (z_1, z_2)^T$를 **은닉층**, 그리고 $\\mathbf{y} = \\hat{y}$를 **출력층**이라고 부른다.\n",
    "이때 각각의 층을 구성하는 노드는 이전 층의 모든 노드를 입력으로 받아 구성되므로 이러한 층 구조를 **Fully-connected layer** 또는 **Dense layer**라고 부른다.\n",
    "일반적으로, 퍼셉트론 구조를 층으로 쌓아 만들어진 네트워크 구조를 통틀어 **(인공) 신경망 (Artificial Neural Network; ANN)** 이라고 부른다.\n",
    "\n",
    "주목할만한 점은 이 모델의 결정경계는 $\\left\\{ \\mathbf{x} \\in  \\mathbb{R}^d : S(H(W\\mathbf{x})) = 0\\right\\}$이므로, 더 이상 입력 $\\mathbf{x}$에 대하여 선형적이지 않다는 점이다.\n",
    "따라서 퍼셉트론을 쌓는 것으로 우리는 비선형성을 기대할 수 있다.\n",
    "그리고 놀라운 사실은 위와 같이 단순히 하나의 은닉층을 추가한 인공 신경망 구조를 임의의 연속함수로 근사시킬 수 있다는 점이다.\n",
    "이를 *Universal Approximation Theorem(UAT)* 이라고 한다.\n",
    "UAT에 관한 더 자세한 내용은 [여기](https://en.wikipedia.org/wiki/Universal_approximation_theorem)를 참고하라.\n",
    "이러한 점에서 ANN을 Universal estimator라고 분류하기도 한다.\n",
    "\n",
    "수학 전공자들에겐 *Stone-Weierstrass Theorem*이 익숙할텐데, UAT는 이와 유사한 성질의 정리다.\n",
    "즉, ANN이 어디로든 수렴할 수 있음은 이야기해주지만, 우리가 원하는 함수로 *어떻게* 수렴시켜야 하는지는 말해주지 않는다.\n",
    "이 *어떻게*에 해당하는 부분은 달리 말해, 최적의 파라미터들 $s_1, s_2, w_{11}, \\ldots, w_{22}$를 찾는 과정이다.\n",
    "이러한 과정을 **신경망의 학습**이라고 한다.\n",
    "\n",
    "\n",
    "\n",
    "> ***Remark )*** 딥러닝에 관련된 많은 문헌들에서는 수학에서와는 달리 입력벡터 $\\mathbf{x}$를 행벡터로 취급하는 경우가 많다. 이는 코딩을 할 때 입력 데이터를 행으로 입력하기가 더 쉽기 때문이기도 하며, 입력에서 출력으로의 방향이 왼쪽에서 오른쪽인것처럼 $\\mathbf{x}W$로 출력을 표현하여 네트워크가 흐르는 방향과 기호들의 방향을 일치시켜주기 위함이기도 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 신경망 학습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
